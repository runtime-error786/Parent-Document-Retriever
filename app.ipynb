{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "# Load PDF documents\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "        docs.extend(pdf_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='Speculative RAG: Enhancing Retrieval Augmented\\nGeneration through Drafting\\nZilong Wang1∗\\nZifeng Wang2\\nLong Le2\\nHuaixiu Steven Zheng3\\nSwaroop Mishra3\\nVincent Perot3\\nYuwei Zhang1\\nAnush Mattapalli4\\nAnkur Taly4\\nJingbo Shang1\\nChen-Yu Lee2\\nTomas Pfister2\\n1University of California, San Diego\\n2Google Cloud AI Research\\n3Google DeepMind\\n4Google Cloud AI\\nAbstract\\nRetrieval augmented generation (RAG) combines the generative abilities of large\\nlanguage models (LLMs) with external knowledge sources to provide more ac-\\ncurate and up-to-date responses. Recent RAG advancements focus on improving\\nretrieval outcomes through iterative LLM refinement or self-critique capabilities\\nacquired through additional instruction tuning of LLMs. In this work, we intro-\\nduce SPECULATIVE RAG – a framework that leverages a larger generalist LM\\nto efficiently verify multiple RAG drafts produced in parallel by a smaller, dis-\\ntilled specialist LM. Each draft is generated from a distinct subset of retrieved\\ndocuments, offering diverse perspectives on the evidence while reducing input\\ntoken counts per draft. This approach enhances comprehension of each subset\\nand mitigates potential position bias over long context. Our method accelerates\\nRAG by delegating drafting to the smaller specialist LM, with the larger generalist\\nLM performing a single verification pass over the drafts. Extensive experiments\\ndemonstrate that SPECULATIVE RAG achieves state-of-the-art performance\\nwith reduced latency on TriviaQA, MuSiQue, PubHealth, and ARC-Challenge\\nbenchmarks. It notably enhances accuracy by up to 12.97% while reducing latency\\nby 51% compared to conventional RAG systems on PubHealth.\\n1\\nIntroduction\\nLarge language models (LLMs) have demonstrated remarkable success in question answering\\ntasks (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023). Trained on massive datasets,\\nLLMs leverage their extensive parametric memory to generate seemingly plausible responses to user\\nqueries (Kojima et al., 2022; Kamalloo et al., 2023). However, when faced with knowledge-intensive\\nquestions demanding up-to-date information or obscure facts (Petroni et al., 2021), LLMs can struggle\\nwith factual inaccuracies and produce hallucinated content (Huang et al., 2023; Xu et al., 2024).\\nRetrieval Augmented Generation (RAG) has emerged as a promising solution to mitigate these\\nissues. By incorporating information retrieved from an external database into the context (Gao\\net al., 2023b), RAG effectively reduces factual errors in knowledge-intensive tasks. This approach\\nnot only enables easy and efficient access to vast databases but also facilitates timely and accurate\\nknowledge integration Due to the inherent limitations in the precision of current dense retrievers and\\nthe vastness of knowledge required to answer complex questions (Chen et al., 2022), RAG systems\\ntypically retrieve multiple documents to ensure the inclusion of all necessary information in the\\ncontext (Petroni et al., 2021). This practice inevitably increases the length of the input to the LLMs,\\n∗Work done while the author was a student researcher at Google Cloud AI Research. Correspondence to:\\nZilong Wang <zlwang@ucsd.edu>, Chen-Yu Lee <chenyulee@google.com>\\nPreprint. Under review.\\narXiv:2407.08223v1  [cs.CL]  11 Jul 2024\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='(a) Standard RAG\\nPrompt\\nQ\\nQuery\\n1\\n2\\n3\\n(Incorporate \\nAll docs into \\nthe prompt)\\nGeneralist\\nLM\\nA\\nFinal \\nAnswer\\n(b) Self-Reﬂective RAG\\nQ\\n1\\n2\\n3\\n4\\nNeed \\nRetrieval\\nRelevant\\nIrrelevant\\nGeneralist\\nLM\\nA\\nSupported\\nRelevant\\nGenerate special tags \\nto reﬂect over the \\nretrieved docs and the \\ngenerated answers\\nSpecial tags \\nlearned during \\ninstruction-tuning\\n(c) Corrective RAG\\n1\\n2\\n3\\nCorrect\\nAmiguous\\nIncorrect\\nNLI Model\\nQ\\n1\\n2\\nQuery\\nRelevant \\nDocs\\nWeb Search\\nGeneralist\\nLM\\nA\\nUse an NLI model to \\nclassify retrieval docs\\nIncorporate relevant docs and the \\ndocs from web search into prompt.\\n(d) Ours: Speculative Retrieval-Augmented Generation (Speculative RAG)\\nQ\\nEnd generation or Continue querying \\nGeneralist LM for other tasks\\n1\\n2\\nSpecialist\\nRAG Drafter\\nQ +\\nQ +\\nQ +\\nEﬃciently generate drafts α and rationale β\\n4\\n5\\n3\\n6\\nUnderstand multiple perspectives of the docs\\n(Docs of the same color are from the same topic cluster)\\nα1 β1\\nα2 β2\\nα3 β3\\nNo need to instruction-\\ntune the Generalist LM\\n       When Generalist LM \\nencounters knowledge-\\nintensive queries?\\n        Evaluate each answer draft based \\non the question and the rationale in \\nparallel using Generalist LM and Accept \\nthe Best Draft.\\nQuery Generalist LM\\nCall Specialist RAG Drafter\\nEvaluate & Accept Drafts into Generalist LM\\nUnderstand documents \\nin parallel with the RAG Drafter\\nA = argmax Score( αi | Q, βi )\\nαi\\nEvaluate drafts by Generalist\\nLM & Accept the best draft\\nβi\\nA=αi\\n…\\n1\\n2\\nRetrieved Documents\\nA\\nEvidence-supported Answers\\nA Knowledge-intensive Query\\nQ\\nA Generalist LM\\nA Specialist LM\\nFigure 1: Illustration of different RAG approaches. Given a knowledge-intensive query Q and\\nretrieved documents, (a) Standard RAG incorporates all documents into the prompt, increasing\\ninput length and slowing inference; (b) Self-Reflective RAG (Asai et al., 2023) requires specialized\\ninstruction-tuning of the general-purpose language model (LM) to generate specific tags for self-\\nreflection; (c) Corrective RAG (Yan et al., 2024) employs an external retrieval evaluator to refine\\ndocument quality, focusing solely on contextual information without enhancing reasoning capabilities;\\n(d) In contrast, our proposed SPECULATIVE RAG leverages a larger generalist LM to efficiently\\nverify multiple RAG drafts produced in parallel by a smaller, specialized LM. Each draft is generated\\nfrom a distinct subset of retrieved documents, providing diverse perspectives on the evidence while\\nminimizing the number of input tokens per draft.\\npresenting significant challenges, particularly since encoding lengthy retrieved documents incurs\\nadditional latency and require more complex reasoning. Recent studies have explored ways to extend\\nthe context length limit of LLMs (Ding et al., 2023; Reid et al., 2024; Ma et al., 2024), yet achieving\\nwell-grounded reasoning over extended contexts remains an open question (Liu et al., 2024; Li et al.,\\n2024). Consequently, striking a balance between efficiency and effectiveness in RAG has become a\\ncentral research question in the literature. Existing work on RAG systems primarily concentrates\\non improving the quality of contextual information in retrieval outcomes, but often neglecting the\\nlatency issues associated with these systems (Ma et al., 2023; Baek et al., 2023; Yan et al., 2024; Xie\\net al., 2023; Asai et al., 2023; Feng et al., 2023). These methods typically rely on multiple refinement\\niterations or customized instruction-tuning for self-critique abilities. Integrating such enhancements\\ninto generic LMs requires additional training or increased latency, posing practical challenges in\\nreal-world applications.\\nTo this end, we introduce SPECULATIVE RAG, a RAG framework designed to offload computational\\nburden to a smaller, specialist LM that serves as an efficient and robust RAG module for existing\\ngeneralist LMs. Inspired by Speculative Decoding (Leviathan et al., 2023; Chen et al., 2023a; Xia\\net al., 2024), which accelerates auto-regressive LM inference by concurrently generating multiple\\ndraft tokens with a smaller model and verifying them in parallel with the base model, our approach\\nadapts this concept to RAG.\\nIn SPECULATIVE RAG, we partition retrieved documents into subsets for drafting answer candidates.\\nWe cluster the retrieved documents by content similarity and sample one document from each cluster\\nto form a subset, minimizing redundancy and maximizing diversity. These document subsets are\\nthen fed to multiple instances of the RAG module, which generate draft answers with corresponding\\nrationales in parallel. This smaller, specialized RAG module, excels at reasoning over retrieved\\ndocuments and can rapidly produce accurate responses. Subsequently, the generalist LM bypasses\\nthe detailed review of potentially repetitive documents, focusing instead on validating the drafts\\n2\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='against the rationales to determine the most accurate answer. We utilize the strong language modeling\\ncapabilities of generalist LMs, calculating the conditional generation probability of the answer drafts\\nand rationales as a confidence score. Our key contributions are:\\n• We introduce a novel RAG framework that employs a smaller specialist RAG drafter to generate\\nhigh-quality draft answers. Each draft is derived from a distinct subset of retrieved documents,\\noffering diverse perspectives while reducing input token counts per draft.\\n• The generalist LM, operating with the RAG drafter, requires no additional tuning. It simply verifies\\nand integrates the most promising draft into the final answer. This approach enhances comprehen-\\nsion of each subset and mitigates potential lost-in-the-middle (Liu et al., 2024) phenomenon.\\n• Our method significantly accelerates RAG by delegating drafting to the smaller specialist LM, with\\nthe larger generalist LM performing a single, unbiased verification pass over the drafts in parallel.\\nExtensive experiments on 4 free-form question-answering and closed-set generation benchmarks\\ndemonstrate the superior effectiveness and efficiency of the method.\\n2\\nRelated Works\\nRetrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by\\nretrieving relevant documents from external databases and incorporating them into the generation\\nprocess (Gao et al., 2023b; Lewis et al., 2020; Khandelwal et al., 2020; Izacard & Grave, 2021;\\nLuo et al., 2023a). Recent work has primarily focused on enabling LLMs to understand when and\\nwhat to retrieve (Ma et al., 2023; Chen et al., 2023b; Jiang et al., 2023b; Schick et al., 2024), or\\ndesigning approaches to better utilize contexts (Yu et al., 2023; Yoran et al., 2023; Wang et al., 2023b;\\nSarthi et al., 2024; Baek et al., 2023; Xu et al., 2023; Kim et al., 2024). Among them, SAIL (Luo\\net al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents. Self-\\nReflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in\\ninstruction-tuning datasets. However, both approaches require additional instruction-tuning of generic\\nLLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b).\\nFurthermore, long context with retrieved documents can suffer from computational inefficiency and\\nposition bias (Liu et al., 2024). Corrective RAG (Yan et al., 2024) on the other hand proposes a\\nlightweight retrieval evaluator, but it lacks the capability for high-level reasoning. In contrast, our\\nproposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model\\nto efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist\\nLMs to verify and integrate.\\nSpeculative Decoding Speculative decoding (Stern et al., 2018; Xia et al., 2023; Chen et al., 2023a;\\nLeviathan et al., 2023; Xia et al., 2024) aims to reduce auto-regressive decoding latency through a\\ndraft-then-verify paradigm. This involves drafting multiple future tokens with a small model and\\nverifying them in parallel with the target model (Xia et al., 2024). The draft model is typically\\neither an independent model from the same series (Leviathan et al., 2023; Chen et al., 2023a) or the\\ntarget model itself (Zhang et al., 2023a; Cai et al., 2024). Our approach extends this concept from\\ntoken-level drafting to answer-level drafting. In contrast to traditional verification criteria (Stern et al.,\\n2018; Xia et al., 2023; Leviathan et al., 2023; Chen et al., 2023a; Miao et al., 2024), which accept or\\nreject tokens based on their generation probabilities, we leverage language modeling objectives to\\ndirectly assess the confidence of entire answer drafts.\\n3\\nSpeculative Retrieval Augmented Generation through Drafting\\nProblem Formulation In knowledge intensive tasks, each entry can be represented as (Q, D, A),\\nwhere Q is a question or statement that requires additional knowledge; D = {d1, ..., dn} is a set\\nof n documents retrieved from the database; A is the expected answer. Particularly, in question\\nanswering tasks, Q and A are the question and the expected answer in natural language form; in the\\nstatement verification tasks, Q is a statement and A ∈{True, False} is a Boolean value indicating\\nthe statement’s correctness; in the multiple choice tasks, Q is a question with a few options and\\nA ∈{A, B, C, ...} is the index of the correct answer. The objective of a RAG system is to generate\\na fluent response containing the expected answer or select the expected answer from the provided\\noptions based on the context provided by the retrieved supporting documents.\\n3\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='3.1\\nOverview\\nWe introduce Speculative Retrieval Augmented Generation (SPECULATIVE RAG), as illustrated in\\nFigure 1. We aim at enhancing the reasoning ability of LLMs over retrieved documents without\\ncompromising processing speed. Instead of relying on brute-force parameter scaling or instruction-\\ntuning an entire LM to handle knowledge-intensive tasks, we propose a divide-and-conquer approach.\\nWe utilize a smaller specialist LM, the RAG drafter, to rapidly generate multiple answer drafts\\nbased on retrieved results. Then, a larger generalist LM, the RAG verifier, assesses these drafts,\\nselects the best one based on its rationale, and integrates it into the generation results.\\nAlgorithm 1: SPECULATIVE RAG\\nData: (Q, D = {di}n\\ni ) is the question and n retrieved documents; m subsets, each containing k\\ndocuments, are sampled from D; k also corresponds to the number of clusters during clustering.\\nResult: ˆ\\nA is the predicted answer to the question.\\n1 Function Speculative RAG (Q, D, m, k):\\n2\\n{c1, c2, ..., ck}\\nK-Means\\n←\\n−\\n−\\n−\\n−C(d1, ..., dn|Q)\\n▷Cluster the documents into k groups using an embedding model C.\\n3\\n∆←{}\\n4\\nrepeat\\n5\\nδj ←{}\\n▷Construct a subset of the retrieved documents δj\\n6\\nfor ci ∈{c1, ..., ck} do\\n7\\nδj = δj ∪{random.sample(ci)}\\n▷Sample one document from each cluster ci into subset δj.\\n8\\nend\\n9\\n∆= ∆∪{δj}\\n10\\nuntil |∆| = m\\n▷Repeat the sampling until there are m unique subsets in total.\\n11\\nfor δj ∈∆do in parallel\\n▷Process m subsets in parallel.\\n12\\nαj, βj ←MDrafter.generate(Q, δj)\\n▷Generate the draft α and rationale β with MDrafter.\\n13\\nρj ←MVerifier.score(αj|Q, βj)\\n▷Compute the confidence score ρ with MVerifier.\\n14\\nend\\n15\\nˆ\\nA ←arg maxαj ρj\\n▷Select the one with the highest score as the final answer.\\n16 return ˆ\\nA\\nSpecifically, as shown in Algorithm 1, we first cluster the retrieved documents with regard to their\\nrelation to the posed question, where each cluster represents one perspective in the retrieval results\\n(Line 2). Then we sample one document from each cluster into a subset so the documents in this\\nsubset covers the multiple perspectives in the retrieval results. We aim at minimizing redundancy and\\nincrease the diversity of the documents (Line 5 to 8). We denote one subset as δ ⊂D that contains\\nretrieved documents with diverse contents and multiple perspectives in the retrieval results. Then, we\\ndistribute each subset δ to a RAG drafter endpoint MDrafter with the posed question Q to generate the\\nanswer draft α and the rationale β in parallel (Line 12). The RAG drafter is instruction-tuned to be\\na specialist in understanding the retrieved documents and produce rationales that are faithful to the\\ninput documents. It is smaller than generalist LMs, and its parallel processing further ensures high\\nefficiency. For each draft-rationale pair (α, β) from MDrafter, we compute a confidence score with\\nthe generalist LM MVerifier based on the question Q and corresponding rationale β (Line 13). It is\\nworth mentioning that MVerifier does not need to be instruction-tuned since we leverage its language\\nmodeling ability already learned during pre-training. Meanwhile, MVerifier can verify the drafts based\\non the informative rationale provided by MDrafter instead of processing tedious or possibly redundant\\nretrieved documents. Finally, we select the answer draft with the highest confidence score as the final\\nanswer and integrate it into the generation results of the generalist LM (Line 15).\\n3.2\\nSpecialist RAG Drafter\\nInstead of tuning a large generalist LM for the RAG scenario, we leverage a smaller specialist LM,\\nMDrafter, to understand retrieved documents. MDrafter is specialized in answering the given question\\nbased on the supporting documents and not expected to cope with general problems. It serves as a\\nRAG module for the generalist LMs when solving knowledge-intensive tasks. We train MDrafter to\\ngenerate both the answer draft and the rationale to better understand the contextual documents.\\nInstruction Tuning Given a triplet (Q, A, D), where Q is a general query, A is the response, and D\\nis a retrieved supporting document, we augment it with the rationale of the response A based on the\\ndocument D. We denote the rationale as E which extracts essential information from the document\\nand explains why the response is reasonable to the query concisely (Hsieh et al., 2023) so it is of\\n4\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='shorter length and delivers information coherent with the original document. We leverage relatively\\nstrong LMs to automatically synthesize the rationale E for each triplet. Specifically, we directly query\\nthe strong LM to understand the knowledge from the document and provide the intermediate rationale\\nbetween the instruction and response. Refer to Appendix A for detailed prompts. After generating the\\nrationale, we finetune a pre-trained LM using the standard language modeling objective, maximizing\\nthe likelihood: E(Q,A,D,E) log PMDrafter(A, E | Q, D), where (Q, A, D, E) is an augmented entry in\\nthe dataset; PMDrafter(A, E | Q, D) is the probability of generating the response and rationale based\\non the query and document. We use this instruction-tuned model as the specialist RAG drafter which\\nlearns to generate a well-grounded response and rationale given the query and relevant documents.\\nMulti-Perspective Sampling For each knowledge-intensive question, we retrieve a set of documents\\nfrom the database using the posed question as the retrieval query. These documents may contain\\ndiverse content due to the ambiguity inherent in the query. To minimize redundancy and enhance\\ndiversity of the document subsets used for generating answer drafts, we employ a multi-perspective\\nsampling strategy. We first cluster the documents into a few topics using an instruction-aware\\nembedding model (Peng et al., 2024) and the K-Means clustering algorithm (Jin & Han, 2011).\\nemb(d1), ..., emb(dn) = E(d1, ..., dn|Q)\\n{c1, ..., ck} = K-Means(emb(d1), ..., emb(dn))\\nδ =\\n\\x08\\nrandom.sample(c) for c ∈{ci}k\\n1\\n\\t\\nwhere E is an instruction-aware embedding model which embeds a string with regard to a provided\\ninstruction (the posed question Q); emb(di) is the embedding for the retrieved document di; cj is a\\ncluster of retrieved documents with similar topics and contents; k is a hyper-parameter that controls\\nthe number of clusters. We sample one document from each cluster into a document subset δ so\\neach subset contains k documents of diverse contents. In total, we construct m subsets for parallel\\ninference with the RAG drafter.\\nRAG Drafting We run MDrafter over the m document subsets and produce corresponding answer\\ndrafts. Refer to Appendix B for detailed prompt. We incorporate each document subset into the\\nprompt and query MDrafter for responses. We obtain m drafts as the answer candidates and each draft\\nis grounded based on the multiple perspectives in the retrieval results. Specifically, given a document\\nsubset δj = {dj1, .., djk}, we query MDrafter in parallel with the following prompt for the answer draft\\nand rationale: Q, dj1, ..., djk →αj, βj, where the prompt contains the posed question Q along with\\nthe document subset; the generation result contains the answer draft α and the rationale β. We denote\\nthe conditional generation probability as ρDraft,j = P(βj|Q, dj1, ..., djk) + P(αj|Q, dj1, ..., djk, βj),\\nwhich measures the reliability of generating rationales and the confidence in producing answer drafts.\\n3.3\\nGeneralist RAG Verifier\\nAfter generating drafts and the rationale from the RAG drafter MDrafter, we evaluate them by a\\ngeneralist LM MVerifier to filter out the less reliable drafts and select the best answer. The generalist\\nLM can be any off-the-shelf pre-trained LM. We only consider the draft-rationale pair (α, β) and\\nskip the tedious and redundant retrieval results. We resort to the language modeling ability of the\\ngeneralist LM to rank and select the draft-rationale pairs.\\nEvaluation Scores First, we calculate the self-consistency score by determining the conditional\\nprobability of generating a draft-rationale pair given the question, ρSelf-contain = P(α, β|Q). This\\nscore helps assess whether the draft and rationale are self-consistent in the context of the question.\\nGiven the characteristics of language modeling, a self-consistent draft-rationale pair is expected to\\nyield a higher probability. Furthermore, we incorporate a self-reflection statement R that prompts\\nMVerifier to assess the reliability of an answer draft (e.g. “Do you think the rationale supports the\\nanswer, yes or no?”). We define the self-reflection score as ρSelf-reflect = P(\"Yes\"|Q, α, β, R)\\nwhere we compute the conditional probability of the positive answer (\"Yes\") to the self-reflection\\nstatement.\\nComputation Method We can efficiently compute the self-consistency and self-reflection scores\\nwithin one forward pass of MVerifier. Given a question Q and a draft-rationale pair (α, β), we\\nconstruct a prompt [Q, α, β, R, \"Yes\"], where R is the self-reflection statement. We encode the\\nprompt with MVerifier, and acquire the probability of each token conditioned on the previous tokens\\nP(ti|t<i). We leverage this auto-regressive feature and aggregate the probability of the relevant\\n5\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='tokens to compute the self-consistent score ρSelf-contain and self-reflection score ρSelf-reflect.\\nQ,\\nρSC\\nz}|{\\nα, β, R,\\nρSR\\nz }| {\\n\"Yes\"\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n−\\n→⇒\\n(\\nρSC = Q\\nti∈α P(ti|t<i) · Q\\nti∈β P(ti|t<i)\\nρSR = Q\\nti∈\"Yes\" P(ti|t<i)\\nFinally, we produce the final score, ρj = ρDraft,j · ρSC,j · ρSR,j, and then select the most reliable\\nanswer as the final answer to the question ˆ\\nA = arg maxαj ρj.\\n4\\nExperiments\\nWe evaluate our proposed SPECULATIVE RAG on four public retrieval augmented generation bench-\\nmarks: TriviaQA (unfiltered) (Joshi et al., 2017), MuSiQue (Trivedi et al., 2022), PubHealth (Zhang\\net al., 2023b), and ARC-Challenge (Clark et al., 2018). TriviaQA and MuSiQue are challenging\\nopen-domain question answering datasets where RAG systems are required to answer questions on\\nfactual knowledge. TriviaQA typically requires one accurate piece of evidence from the documents,\\nwhereas MuSiQue demands multiple documents to construct a multi-hop reasoning chain. Following\\nprevious works (Guu et al., 2020; Asai et al., 2023; Yan et al., 2024), we evaluate performance of the\\nfree-form generation based on whether gold answers are contained within the generated response\\nor not. PubHealth and ARC-Challenge are closed-set generation datasets. PubHealth is a dataset of\\nmedical claims spanning a variety of biomedical subjects and it requires the RAG system to verify a\\ngiven claim based on the retrieved documents. ARC-Challenge introduces a multi-choice question\\nanswering dataset, composed of science exam questions from grade 3 to grade 9. For closed-set\\ngeneration tasks, we use accuracy metrics to evaluate whether the generated answers match the\\nground truth.\\n4.1\\nBaselines\\nStandard RAG For standard RAG, we incorporate all the retrieved documents into the prompt\\nas contextual information. Refer to Appendix C for detailed prompts. We run standard RAG\\nexperiments on off-the-shelf LLMs including Mistral7B, Mistral-Instruct7B (Jiang et al., 2023a),\\nMixtral8x7B, Mixtral-Instruct8x7B (Jiang et al., 2024), and Alpaca7B (Dubois et al., 2024). We also\\ninclude the performance of Toolformer (Schick et al., 2024) and SAIL (Luo et al., 2023a) which are\\noriginally reported from Asai et al. (2023). Toolformer7B is an LM instruction-tuned to use tools\\nincluding a search engine, and SAIL7B is an LM instruction-tuned on the Alpaca instruction tuning\\nset augmented with search results from different sources such as DuckDuckGo and Wikipedia.\\nSelf-Reflective RAG and Corrective RAG Self-Reflective RAG (Self-RAG) (Asai et al., 2023)\\nand Corrective RAG (CRAG) (Yan et al., 2024) are more advanced RAG systems that enhances\\nthe quality of contextual information in the retrieval results. CRAG introduces an external evaluator\\nto assess the quality of retrieved documents, and to refine them before the response generation.\\nSelf-RAG instruction-tunes an LM to generate special self-refection tags. These tags guides the LM\\nto dynamically retrieve documents when necessary, critique the retrieved documents relevance before\\ngenerating responses. Self-CRAG is to apply the Self-RAG approach on the refined documents of\\nCRAG. We adopt the same backbone LLMs across all methods as our proposed SPECULATIVE RAG\\nfor fair comparisons.\\n4.2\\nExperiment Settings\\nIn our experiments, we utilize Mistral7B (v0.1) as our base LM for the RAG drafter. For RAG\\nverifier, we employ either Mistral7B (v0.1) or Mixtral8x7B (v0.1) without any fine-tuning, denoted as\\nMVerifier-7B or MVerifier-8x7B. We pre-compute embeddings of retrieved documents using a lightweight\\ninstruction-aware embedding model InBedderRoberta (Peng et al., 2024) as part of the retrieval process.\\nInference is conducted using the vLLM framework (Kwon et al., 2023) with greedy decoding\\n(temperature = 0). We adopt the same experiment settings from Asai et al. (2023) and include a more\\nchallenging benchmark, MuSiQue (Trivedi et al., 2022). Our focus is on RAG reasoning rather than\\nevidence citation, so we omit the other two long-form generation benchmarks, Biography (Min et al.,\\n2023) and ALCE-ASQA (Gao et al., 2023a). On TriviaQA, PubHealth, and ARC-Challenge, we\\nretrieve top 10 documents and generate 5 drafts per query (m = 5), with each draft based on a subset\\nof 2 documents (k = 2). For the MuSiQue dataset, we retrieve top 15 documents and generate 10\\n6\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='Table 1: Retrieval augmentation generation results on TriviaQA, MuSiQue, PubHealth, and ARC-\\nChallenge (ARC-C). (∗We use the RAG drafter’s generation probability ρDraft as the confidence score\\nfor selecting drafts when we use it alone; † indicates numbers reported in Asai et al. (2023); −\\ndenotes numbers that are not reported by the original papers or are not applicable; ‡we use Mistral7B\\nor Mixtral8x7B as the RAG verifier, and denote them as MVerifier-7B or MVerifier-8x7B.)\\nRAG Method\\nFree-form\\nClosed-set\\nTriviaQA\\nMuSiQue\\nPubHealth\\nARC-C\\nStandard RAG\\nMistral7B (Jiang et al., 2023a)\\n54.15\\n16.71\\n34.85\\n42.75\\nMixtral8x7B (Jiang et al., 2024)\\n59.85\\n19.16\\n37.08\\n48.72\\nMistral-Instruct7B (Jiang et al., 2023a)\\n67.11\\n17.99\\n42.15\\n47.70\\nMixtral-Instruct8x7B (Jiang et al., 2024)\\n73.91\\n29.42\\n63.63\\n78.41\\nAlpaca7B (Dubois et al., 2024)†\\n64.1\\n-\\n40.2\\n48.1\\nToolformer6B (Schick et al., 2024)†\\n48.8\\n-\\n-\\n-\\nSAIL7B (Luo et al., 2023a)†\\n-\\n-\\n69.2\\n48.4\\nSelf-Reflective RAG & Corrective RAG\\nCRAGMistral-7B (Yan et al., 2024)\\n-\\n-\\n59.04\\n74.87\\nSelf-RAGMistral-7B (Asai et al., 2023)\\n64.84\\n21.72\\n72.44\\n74.91\\nSelf-CRAGMistral-7B (Yan et al., 2024)\\n-\\n-\\n72.85\\n75.26\\nOur Speculative RAG\\nMDrafter-7B\\n∗\\n71.11\\n27.89\\n75.58\\n74.49\\nMVerifier-7B\\n‡ + MDrafter-7B\\n73.91\\n31.03\\n75.79\\n76.19\\nMVerifier-8x7B\\n‡ + MDrafter-7B\\n74.24\\n31.57\\n76.60\\n80.55\\ndrafts for each query (m = 10), each using a subset of 6 documents due to more complex reasoning.\\nFurther details regarding instruction-tuning can be found in Appendix E.\\n4.3\\nMain Results\\nWe compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced\\nSelf-Reflective RAG and Corrective RAG on four datasets: TriviaQA, MuSiQue, PubHealth, and\\nARC-Challenge. We report the performance of MDrafter-7B when used alone or paired with the RAG\\nverifier (e.g. MVerifier-7B, MVerifier-8x7B). Following prior work (Asai et al., 2023; Yan et al., 2024),\\nwe report accuracy as the performance metric.\\nSuperior Performance over Baselines Table 1 demonstrates that SPECULATIVE RAG consis-\\ntently outperforms all baselines across all four benchmarks. Particularly, MVerifier-8x7B + MDrafter-7B\\nsurpasses the most competitive standard RAG model, Mixtral-Instruct8x7B, by 0.33% on TriviaQA,\\n2.15% on MuSiQue, 12.97% on PubHealth, and 2.14% on ARC-Challenge. With a comparable\\nnumber of instruction-tuned parameters, MVerifier-7B + MDrafter-7B outperforms all Self-Reflective and\\nCorrective RAG methods, and MDrafter alone can surpass these baselines in most settings.\\nEffective Instruction Tuning for RAG Drafter Our instruction tuning is effective in enhancing\\nthe reasoning ability of the drafter model (Hsieh et al., 2023), as we observe a remarkable perfor-\\nmance improvement comparing Mistral7B and MDrafter-7B. Moreover, the performance of Mixtral8x7B\\nsignificantly improves when paired with the instruction-tuned RAG drafter MDrafter-7B, showing\\ngains of 14.39% on TriviaQA, 12.41% on MuSiQue, 39.52% on PubHealth, and 31.83% on ARC-\\nChallenge. Similar improvements are observed with Mistral7B as well. For Mistral7B, we observed\\nimprovements of 19.76% on TriviaQA, 14.32% on MuSiQue, 40.94% on PubHealth, and 33.44%\\non ARC-Challenge. We attribute these improvements to the superior reasoning capabilities of the\\nRAG drafter over the retrieved documents in SPECULATIVE RAG. By minimizing the redundancy\\nin the sampled documents, the RAG drafter generates higher quality answer drafts based on diverse\\nperspectives from the retrieval results.\\nReliable Scoring by RAG Verifier The reliable draft verification by the generalist LM also con-\\ntributes to the enhanced performance. The performance improves remarkably comparing MDrafter-7B\\nand MVerifier-7B + MDrafter-7B. The instruction-tuned RAG drafter is specialized in generating answer\\ndrafts based on the retrieved documents while the language modeling capabilities of generic LMs are\\nleveraged to validate each draft in light of its rationale. This method is both effective and easy to\\nimplement, showcasing the effectiveness of this verification approach.\\n7\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='TriviaQA\\nMuSiQue\\nPubHealth\\nARC-Challenge\\nDataset\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nLatency (s)\\n+31%\\n+21%\\n+105%\\n+36%\\n2.52\\n2.72\\n2.40\\n2.02\\n+34%\\n+30%\\n+129%\\n+31%\\n2.59\\n2.93\\n2.68\\n1.94\\n+50%\\n+33%\\n+134%\\n+48%\\n2.90\\n2.99\\n2.74\\n2.19\\n1.93\\n2.25\\n1.17\\n1.48\\nStandard RAG: Mixtral-Instruct-8x7B (TP=4)\\n(TP=8)\\n(TP=16)\\nSpeculative RAG: Mixtral-8x7B w/ RAG Drafter (7B)\\nFigure 2: Latency analysis of Standard RAG: Mixtral-Instruct8x7B with tensor parallelism and\\nSPECULATIVE RAG: MVerifier-8x7B + MDrafter-7B on TriviaQA, MuSiQue, PubHealth, and ARC-\\nChallenge. The latency difference between Standard RAG and SPECULATIVE RAG is highlighted in\\nred (+x%). TP indicates the tensor parallelism size when running Mixtral-Instruct8x7B for Standard\\nRAG. The latency varies across different datasets due to different retrieved document lengths.\\nSPECULATIVE RAG encodes the retrieved documents in parallel and generates answer drafts with a\\nsmaller RAG drafter. This significantly improves the efficiency over Standard RAG.\\n4.4\\nLatency Analysis\\nWe analyze the latency of Standard RAG and our SPECULATIVE RAG on TriviaQA, MuSiQue,\\nPubHealth, and ARC-Challenge. We randomly sample 100 cases from each dataset and report\\nthe average time cost for each case, as shown in Figure 2. To simulate real-world application\\nscenarios, we process cases individually without batching. As representative example, we run\\nMVerifier-8x7B + MDrafter-7B for SPECULATIVE RAG and Mixtral-Instruct8x7B for Standard RAG, as\\nthese demonstrate the highest performance among competitive baselines (see Table 1). We launch 5\\nendpoints of MDrafter-7B for parallel drafting on TriviaQA, PubHealth, and ARC-Challenge. We launch\\n10 endpoints for MuSiQue due to more drafts. We use tensor parallelism to fit Mixtral-Instruct8x7B\\ninto the GPU memory. We report the latency of Mixtral-Instruct8x7B under tensor parallelism sizes\\nof 4, 8, 16. Increasing tensor parallelism does not improve efficiency due to overheads in tensor\\naggregation and communication. In contrast, SPECULATIVE RAG, with its smaller RAG drafter and\\nparallel draft generation, consistently achieves the lowest latency across all datasets. Particularly, it\\nreduces latency by up to 23.41% on TriviaQA, 17.28% on MuSiQue, 51.25% on PubHealth, and\\n26.73% on ARC-Challenge. This highlights the advantage of our approach in reducing processing\\ntime while maintaining high performance.\\n4.5\\nAblation Studies\\nWe conduct ablation studies on the key components of SPECULATIVE RAG during both drafting and\\nverification stages on TriviaQA and PubHealth in Table 2. We use MVerifier-8x7B + MDrafter-7B as a\\nrunning configuration. Same as the main results, we report the accuracy as performance metrics.\\nDiversity and reduced redundancy in retrieval improves draft quality significantly.\\nIn the\\nfirst set of experiments, we evaluate the impact of multi-perspective sampling during the drafting.\\nRecall that SPECULATIVE RAG clusters retrieved documents into distinct perspectives and sample\\none document from each cluster to reduce redundancy for the draft generation. We compare this\\nagainst two alternative sampling strategies: (1) Random sampling without clustering, where we\\nrandomly select a document subset as context, and (2) Sampling from the same cluster, where we\\nselect all documents from a single cluster. Our results indicate that our proposed sampling method\\nyields the best performance thanks to its ability to leverage diverse context. Particularly, it improves\\nthe accuracy up to 1.88% on TriviaQA and 2.23% on PubHealth. While random sampling without\\nclustering introduces diversity, it is prone to including redundant documents, degrading draft quality.\\nSampling from the same cluster significantly underperforms due to a lack of diverse perspectives.\\nScoring method on self-consistency and self-reflection refines draft quality effectively. In the\\nsecond set of experiments, we examine the scoring method during verification. We remove each of\\nthe specific confidence scores, ρDraft, ρSelf-contain, or ρSelf-reflect in turn. Performance drops are observed\\nwhen any score is removed. Particularly, removing ρDraft leads to a minimal decline, 0.19% on\\nTriviaQA and 1.12% on PubHealth, likely due to the limited verification capability of the smaller\\nRAG drafter. Removing either ρSelf-contain or ρSelf-reflect results in similar performance decreases,\\naround 2.0% on TriviaQA and around 0.8% on PubHealth, indicating that both self-containment and\\nself-reflection capture different key aspects of reasoning and are crucial during verification. Random\\n8\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='Appendix\\nA\\nPrompt of Rationale Generation\\n========================================= Prompt =========================================\\n# Memorize this piece of evidence in mind and use it as if you already know it.\\n# Evidence: State religion\\nDespite enjoying considerable popular support, Christianity was still not the official\\nstate religion in Rome, although it was in some neighboring states such as Armenia, Iberia,\\nand Aksum. Roman Religion (Neoplatonic Hellenism) was restored for a time by the Emperor\\nJulian from 361 to 363. Julian does not appear to have reinstated the persecutions of the\\nearlier Roman emperors. Catholic Christianity, as opposed to Arianism and other ideologies\\ndeemed heretical, was declared to be the state religion of the Roman Empire on 27 February\\n380 by the decree \"De fide catolica\".\\n# Instruction: When did christianity become official religion of Rome?\\n# Response: 380 AD\\n# Rationale: (complete the following sentence with details from the evidence; you can only\\nuse the information from the evidence)\\n======================================= Completion =======================================\\nCatholic Christianity was declared the official state religion of the Roman Empire on\\nFebruary 27th, 380, through the decree \"De fide catolica\".\\nFigure 4: Prompt of Rationale Generation for Gemini-Ultra\\nB\\nPrompt of RAG Drafting\\n========================================= Prompt =========================================\\nResponse to the instruction. Also provide rationale for your response.\\n## Instruction: In Buddhism, what is the state of blissful repose or absolute existence by\\nsomeone relieved of the necessity of rebirth?\\n## Evidence:\\n[1] Buddhism\\nNirvana literally means \"blowing out, quenching, becoming extinguished\". In early Buddhist\\ntexts, it is the state of restraint and self-control that leads to the \"blowing out\" and\\nthe ending of the cycles of sufferings associated with rebirths and redeaths. Many later\\nBuddhist texts describe nirvana as identical with \"anatta\" with complete \"emptiness,\\nnothingness\". In some texts, the state is described with greater detail, such as passing\\nthrough the gate of emptiness (\"sunyata\") realizing that there\"\\n[2] Salvation\\nIt includes a variety of disciplines, such as yoga and meditation. Nirvana is the profound\\npeace of mind that is acquired with moksha (liberation). In Buddhism and Jainism, it is the\\nstate of being free from suffering. In Hindu philosophy, it is union with the Brahman (\\nSupreme Being). The word literally means \"blown out\" (as in a candle) and refers, in the\\nBuddhist context, to the blowing out of the fires of desire, aversion, and delusion, and\\nthe imperturbable stillness of mind acquired thereafter. In Theravada Buddhism the emphasis\\nis on one’s\\n======================================= Completion =======================================\\n## Rationale: Nirvana literally means ’blowing out, quenching, becoming extinguished’. It\\nis described as a state of \"restraint and self-control\" that leads to the \"blowing out\" and\\nthe ending of the cycles of sufferings associated with rebirths and redeaths.\\n## Response: In Buddhism, the state of blissful repose or absolute existence by someone\\nrelieved of the necessity of rebirth is called Nirvana.\\nFigure 5: Prompt of RAG Drafting\\n14\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='C\\nPrompt of Standard RAG\\n========================================= Prompt =========================================\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.\\n### Evidence:\\n[1] Britain (place name)\\nBritain, after which \"Britain\" became the more commonplace name for the island called Great\\nBritain. After the Anglo-Saxon period, \"Britain\" was used as a historical term only.\\nGeoffrey of Monmouth in his pseudohistorical \"Historia Regum Britanniae\" ...\\n[2] Great Britain\\nThe peoples of these islands of \"Prettanike\" were called the \"Priteni\" or \"Pretani\". \"\\nPriteni\" is the source of the Welsh language term Prydain, \"Britain\", which has the same\\nsource as the Goidelic term Cruithne used to refer to the early Brythonic-speaking\\ninhabitants of Ireland. The latter were later called Picts or Caledonians ...\\n...\\n[10] Albion\\nAlbion is an alternative name for Great Britain. The oldest attestation of the toponym\\ncomes from the Greek language. It is sometimes used poetically and generally to refer to\\nthe island, but is less common than ’Britain’ today. The name for Scotland in most of the\\nCeltic languages is related to Albion: \"Alba\" in Scottish Gaelic, \"Albain\" ...\\n### Instruction: What was Britain called - before it was Britain?\\n### Response:\\nFigure 6: Prompt of Standard RAG for Non-instruction-tuned LM\\n========================================= Prompt =========================================\\n[INST] Below is an instruction that describes a task. Write a response for it and state\\nyour explanation supporting your response.\\n### Instruction: What was Britain called - before it was Britain?\\n### Evidence:\\n[1] Britain (place name)\\nBritain, after which \"Britain\" became the more commonplace name for the island called Great\\nBritain. After the Anglo-Saxon period, \"Britain\" was used as a historical term only.\\nGeoffrey of Monmouth in his pseudohistorical \"Historia Regum Britanniae\" ...\\n[2] Great Britain\\nThe peoples of these islands of \"Prettanike\" were called the \"Priteni\" or \"Pretani\". \"\\nPriteni\" is the source of the Welsh language term Prydain, \"Britain\", which has the same\\nsource as the Goidelic term Cruithne used to refer to the early Brythonic-speaking\\ninhabitants of Ireland. The latter were later called Picts or Caledonians ...\\n...\\n[10] Albion\\nAlbion is an alternative name for Great Britain. The oldest attestation of the toponym\\ncomes from the Greek language. It is sometimes used poetically and generally to refer to\\nthe island, but is less common than ’Britain’ today. The name for Scotland in most of the\\nCeltic languages is related to Albion: \"Alba\" in Scottish Gaelic, \"Albain\" ...\\n[/INST] The response is:\\nFigure 7: Prompt of Standard RAG for Instruction-tuned LM\\n15\\n'),\n",
       " Document(metadata={'source': './data/2407.08223v1_removed.pdf', 'file_path': './data/2407.08223v1_removed.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240824130954Z', 'trapped': ''}, page_content='D\\nCase Study\\nFigure 8 shows two drafts generated for the same question. We observe that our RAG drafter can\\nwell understand the multiple perspectives in the retrieval results and generate high-quality drafts. Our\\nRAG verifier can also help filter out the unreliable drafts as we observe a relatively low scores in the\\nfirst draft in Figure 8.\\n=================================== Prompt of Drafter ====================================\\nResponse to the instruction. Also provide rationale for your response.\\n## Instruction: Which actress/singer starred as Doralee Rhodes in the 1980 film, \"Nine to\\nFive\"?\\n## Evidence:\\n[1] Diana DeGarmo\\nSheila until the show’s closure on June 27, 2010. DeGarmo played the role of Doralee Rhodes\\nin the national tour of \"9 to 5\", which was launched in Nashville on September 21, 2010.\\nShe ended her run as Doralee after the July 2011 Minneapolis tour stop. DeGarmo appeared on\\nmusical theatre composer Scott Alan’s third album, ...\\n[2] 9 to 5 (musical)\\nPippa Winslow as Violet Newstead, Louise Olley as Doralee Rhodes and Leo Sene as Franklin\\nHart Jr, with Samantha Giffard as Roz, Matthew Chase as Joe and Mark Houston, Rachel Ivy,\\nand Blair Anderson. \"9 to 5\" will play in the West End at the Savoy Theatre from January 29\\nto August 31, 2019. The production stars Amber Davies (Judy), ...\\n====================================== Completion ========================================\\n## Rationale: Diana DeGarmo played the role of Doralee Rhodes in the national tour of \"9 to\\n5\", which began in September 2010.\\n## Response: Diana DeGarmo\\n====================================== Verification ======================================\\ndraft score: 0.6594\\nself-containment score: 0.3417\\nself-reflection score: 0.5238\\n------------------------------------------------------------------------------------------\\n=================================== Prompt of Drafter ====================================\\nResponse to the instruction. Also provide rationale for your response.\\n## Instruction: Which actress/singer starred as Doralee Rhodes in the 1980 film, \"Nine to\\nFive\"?\\n## Evidence:\\n[1] Cynthia Rhodes\\nRhodes garnered her first non-dance related role in Michael Crichton’s 1984 science fiction\\nthriller \"Runaway\" with Tom Selleck, Kirstie Alley and Gene Simmons. Her most notable role\\nwas as dance instructor Penny Johnson in the hit 1987 motion picture \"Dirty Dancing\" with\\nJennifer Grey and Patrick Swayze. Rhodes’ final motion picture role was the character of\\nVickie Phillips opposite actor Jameson Parker in the sleeper action-adventure movie ...\\n[2] Dolly Parton\\nParton’s feature film \"9 to 5\" (1980). The musical ran at the Ahmanson Theatre, Los Angeles\\nin late 2008. It opened on Broadway at the Marquis Theatre in New York City, on April 30,\\n2009, to mixed reviews. The title track of her 2008 \"Backwoods Barbie\" album was written\\nfor the musical’s character Doralee. Although her score (as well as the musical debut of\\nactress Allison Janney) were praised, ...\\n====================================== Completion ========================================\\n## Rationale: Dolly Parton starred in the 1980 film \"9 to 5\" as Doralee Rhodes.\\nAdditionally, Parton’s 2008 musical \"9 to 5\" was based on the film, with Parton reprising\\nher role as Doralee.\\n## Response: Dolly Parton starred as Doralee Rhodes in the 1980 film, \"Nine to Five\".\\n====================================== Verification ======================================\\ndraft score: 0.71\\nself-containment score: 0.4346\\nself-reflection score: 0.7449\\nFigure 8: Case study of SPECULATIVE RAG from TriviaQA where Dolly Parton is the corrrect\\nanswer.\\n16\\n')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# directly divide in child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(store.yield_keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
